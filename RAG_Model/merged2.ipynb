{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merged RAG Notebook — Road Safety Intervention GPT (NRSH 2025)\n",
    "\n",
    "This notebook is the **final merged version** you asked for. It combines:\n",
    "\n",
    "- Chunked retrieval (better FAISS retrieval using text splitting)\n",
    "- Topic-wise synthesis and structured output (Problem → IRC Clauses → Topic-wise Interventions → Steps → Cost → Compliance)\n",
    "- Strict metadata citations (Option A)\n",
    "- Interactive `process_query` function (Option 1 format)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 — Quick notes\n",
    "- Put your `knowledge_base.json` in the same folder as this notebook.\n",
    "- Do NOT hardcode secrets in the notebook. Set your Hugging Face token as an environment variable:\n",
    "    export HF_TOKEN=\"<your_token>\"\n",
    "  or set HF_TOKEN in your session before running the LLM-loading cell.\n",
    "- Install required libraries (run once):\n",
    "    %pip install langchain langchain-community sentence-transformers faiss-cpu transformers torch accelerate huggingface_hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 — Imports & Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ram/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# Langchain + embeddings + FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Avoid tokenizer parallelism warnings\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Config\n",
    "JSON_FILE_PATH = \"knowledge_base.json\"\n",
    "EMBEDDING_MODEL = \"BAAI/bge-large-en-v1.5\"\n",
    "HF_MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "HF_TOKEN = os.getenv('HF_TOKEN', None)  # set this in your environment\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else (\"mps\" if getattr(torch.backends, 'mps', False) and torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "CHUNK_SIZE = 600\n",
    "CHUNK_OVERLAP = 120\n",
    "TOP_K = 5\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 — Load & Normalize `knowledge_base.json`\n",
    "This cell can handle both the IRC table-style items and the intervention-style items you showed earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 240 documents\n",
      "Intervention/Standard: STOP Sign\n",
      "\n",
      "The 'STOP' sign, used on Minor Roads intersecting Major Roads, requires vehicles to stop before entering and proceed only when safe. It is octagonal with a red background, a white border, and \"STOP\" written centrally in white. Installed on the left side of the approach, it should be placed close to the stop line, typically 1.5 m in advance, without impairing visib\n"
     ]
    }
   ],
   "source": [
    "def load_and_prepare_documents(file_path: str) -> List[Document]:\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\n",
    "            f\"{file_path} not found. Place your knowledge base JSON in the same directory.\"\n",
    "        )\n",
    "\n",
    "    doc_list = []\n",
    "\n",
    "    for item in data:\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # CASE 1 — Gold-standard merged format\n",
    "        # -------------------------------------------------\n",
    "        if 'intervention_name' in item and 'content' in item:\n",
    "            page_content = (\n",
    "                f\"Intervention/Standard: {item['intervention_name']}\\n\\n\"\n",
    "                f\"{item['content']}\"\n",
    "            )\n",
    "\n",
    "            metadata = item.get('metadata', {})\n",
    "            final_metadata = dict(metadata)\n",
    "            final_metadata.setdefault('id', item.get('id', 'N/A'))\n",
    "            final_metadata.setdefault(\n",
    "                'source_reference',\n",
    "                metadata.get('source_reference', item.get('source_reference', 'N/A'))\n",
    "            )\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # CASE 2 — IRC table-style (s_no + data)\n",
    "        # -------------------------------------------------\n",
    "        elif 's_no' in item and 'data' in item:\n",
    "            page_content = (\n",
    "                f\"Standard Type: {item.get('type', 'N/A')}\\n\"\n",
    "                f\"Specification: {item.get('data', 'N/A')}\"\n",
    "            )\n",
    "\n",
    "            final_metadata = {\n",
    "                'id': item.get('s_no', 'N/A'),\n",
    "                'source_reference': item.get('code', 'IRC'),\n",
    "                'irc_clause': f\"{item.get('code', 'IRC')}, Clause {item.get('clause', 'N/A')}\",\n",
    "                'type': 'Standard',\n",
    "                'category': item.get('category', 'N/A'),\n",
    "            }\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # CASE 3 — Intervention entries (intervention + description)\n",
    "        # -------------------------------------------------\n",
    "        elif 'intervention' in item and 'description' in item:\n",
    "            content_text = (\n",
    "                f\"{item['description']}\\n\"\n",
    "                f\"When to Apply: {item.get('when_to_apply', 'N/A')}\\n\"\n",
    "                f\"Why it Works: {item.get('why_it_works', 'N/A')}\"\n",
    "            )\n",
    "\n",
    "            page_content = (\n",
    "                f\"Intervention: {item['intervention']}\\n\\n\"\n",
    "                f\"{content_text}\"\n",
    "            )\n",
    "\n",
    "            final_metadata = {\n",
    "                'id': item.get('id', 'N/A'),\n",
    "                'source_reference': item.get('source', 'N/A'),\n",
    "                'type': 'Intervention',\n",
    "                'category': item.get('category', 'N/A'),\n",
    "            }\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # CASE 4 — Fallback\n",
    "        # -------------------------------------------------\n",
    "        else:\n",
    "            text = json.dumps(item, ensure_ascii=False)\n",
    "            page_content = f\"Document:\\n{text[:1000]}\"\n",
    "\n",
    "            final_metadata = {\n",
    "                'id': item.get('id', 'N/A'),\n",
    "                'source_reference': item.get('source', 'N/A')\n",
    "            }\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Append final doc\n",
    "        # -------------------------------------------------\n",
    "        doc_list.append(\n",
    "            Document(page_content=page_content, metadata=final_metadata)\n",
    "        )\n",
    "\n",
    "    return doc_list\n",
    "\n",
    "\n",
    "# Load documents\n",
    "documents = load_and_prepare_documents(JSON_FILE_PATH)\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(documents[0].page_content[:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 — Chunking & Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7q/cwlrl67n1p3fsjh24d6dk5r00000gn/T/ipykernel_22969/3033526530.py:16: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store built.\n"
     ]
    }
   ],
   "source": [
    "# Chunk the combined documents but preserve metadata\n",
    "chunker = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "chunks, metas = [], []\n",
    "for doc in documents:\n",
    "    splits = chunker.split_text(doc.page_content)\n",
    "    for s in splits:\n",
    "        chunks.append(s)\n",
    "        # copy metadata and keep id/source_reference\n",
    "        m = dict(doc.metadata)\n",
    "        m.setdefault('topic', 'General')\n",
    "        metas.append(m)\n",
    "\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "\n",
    "# Create vector store\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "vector_store = FAISS.from_texts(chunks, embeddings, metadatas=metas)\n",
    "print(\"Vector store built.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 — LLM Loader (no hardcoded tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: meta-llama/Llama-3.2-3B-Instruct on device: mps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ram/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.51s/it]\n",
      "Device set to use mps:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "def load_llm(\n",
    "    model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    token: str = \"\"   # ← put your HF token directly here\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads the Llama 3.2 model using the token directly passed into the function.\n",
    "    \"\"\"\n",
    "\n",
    "    if token == \"YOUR_TOKEN_HERE\" or not token:\n",
    "        raise ValueError(\"❌ ERROR: Please replace 'YOUR_TOKEN_HERE' with your actual HF token.\")\n",
    "\n",
    "    print(f\"Loading model: {model_name} on device: {DEVICE} ...\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        token=token\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    "        torch_dtype=torch.bfloat16 if DEVICE == \"cuda\" else None,\n",
    "        use_auth_token=token\n",
    "    )\n",
    "\n",
    "    # Fix pad token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Create pipeline\n",
    "    llm = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=900,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    print(\"✅ LLM loaded successfully.\")\n",
    "    return llm, tokenizer\n",
    "\n",
    "\n",
    "# Load model\n",
    "llm, tokenizer = load_llm()\n",
    "\n",
    "if not llm:\n",
    "    print(\"LLM not loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 — Intent Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_intent_simple(query: str) -> str:\n",
    "    q = query.lower()\n",
    "    if 'summarize' in q or 'summary' in q:\n",
    "        return 'request_summary'\n",
    "    if any(x in q for x in ['cost', 'estimate', 'price', 'how much']):\n",
    "        return 'cost_estimate'\n",
    "    if any(x in q for x in ['fix', 'intervention', 'solution', 'what should i do', 'how to fix']):\n",
    "        return 'find_intervention'\n",
    "    if any(x in q for x in ['standard', 'specification', 'clause', 'irc', 'rule', 'compliance']):\n",
    "        return 'find_standard'\n",
    "    if any(x in q for x in ['compare', 'difference between', 'vs ', 'v/s']):\n",
    "        return 'compare_interventions'\n",
    "    if any(x in q for x in ['quiz', 'test me']):\n",
    "        return 'request_quiz'\n",
    "    return 'ask_question'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 — Citation Formatter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_citation(metadata: Dict[str, Any]) -> str:\n",
    "    # Strict metadata citation preference: 'irc_clause' -> 'source_reference' + 'id' -> fallback\n",
    "    if not metadata:\n",
    "        return '[Source: N/A]'\n",
    "    if metadata.get('irc_clause'):\n",
    "        return f\"[{metadata['irc_clause']}]\"\n",
    "    parts = []\n",
    "    if metadata.get('source_reference') and metadata['source_reference'] != 'N/A':\n",
    "        parts.append(str(metadata['source_reference']))\n",
    "    if metadata.get('id') and metadata['id'] != 'N/A':\n",
    "        parts.append(str(metadata['id']))\n",
    "    if parts:\n",
    "        return '[' + ', '.join(parts) + ']'\n",
    "    return '[Source: N/A]'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 — Synthesis Prompt & Generation\n",
    "This cell creates the topic-wise + structured prompt (Option 1) and invokes the LLM. If `llm` is None, it returns the retrieval results for inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_topic_wise(llm, tokenizer, retrieved_chunks: List[Dict[str, Any]], query: str, intent: str):\n",
    "    # ----------------------------------------------\n",
    "    # Build Context Blocks\n",
    "    # ----------------------------------------------\n",
    "    blocks = []\n",
    "    citations = []\n",
    "\n",
    "    for chunk in retrieved_chunks:\n",
    "        meta = chunk.metadata if hasattr(chunk, \"metadata\") else chunk.get(\"metadata\", {})\n",
    "        text = chunk.page_content if hasattr(chunk, \"page_content\") else chunk.get(\"text\", \"\")\n",
    "\n",
    "        cite = format_citation(meta)\n",
    "        citations.append(cite)\n",
    "\n",
    "        block = (\n",
    "            f\"---\\n\"\n",
    "            f\"Citation: {cite}\\n\"\n",
    "            f\"Topic: {meta.get('topic', 'N/A')}\\n\"\n",
    "            f\"Metadata: {json.dumps(meta)}\\n\"\n",
    "            f\"Content: {text}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    context_text = \"\\n\".join(blocks)\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # Build Prompt Template\n",
    "    # ----------------------------------------------\n",
    "    template = f\"\"\"\n",
    "You are an Expert Road Safety Analyst. Use ONLY the provided context blocks to answer. Do NOT hallucinate.\n",
    "Produce a TOPIC-WISE and STRUCTURED response with these exact sections:\n",
    "\n",
    "### 1) Problem Interpretation\n",
    "\n",
    "### 2) Applicable IRC Clauses / Sources\n",
    "\n",
    "### 3) Topic-wise Recommended Interventions\n",
    "\n",
    "### 4) Why This Works\n",
    "\n",
    "### 5) Step-by-Step Fix Guide\n",
    "\n",
    "### 6) Estimated Cost (label as ESTIMATE if not in context)\n",
    "\n",
    "### 7) Compliance Check\n",
    "\n",
    "### 8) Final Summary\n",
    "\n",
    "Context Blocks:\n",
    "{context_text}\n",
    "\n",
    "User Query:\n",
    "{query}\n",
    "\n",
    "Notes:\n",
    "- Every factual claim must include inline citations from the context (e.g., [IRC:67-2022, Clause 12.3] or [Source_ID]).\n",
    "- If information is missing, say: \"Cannot answer from provided knowledge base.\"\n",
    "\"\"\"\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # If LLM Not Loaded → Only Show Retrieved Context\n",
    "    # ----------------------------------------------\n",
    "    if not llm:\n",
    "        return \"LLM not loaded. Showing retrieved context only.\", citations\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # Generate Output From LLM\n",
    "    # ----------------------------------------------\n",
    "    output = llm(template, num_return_sequences=1)\n",
    "    raw = output[0][\"generated_text\"]\n",
    "\n",
    "    # Some pipelines echo the prompt; strip if needed\n",
    "    if raw.startswith(template):\n",
    "        answer = raw[len(template):].strip()\n",
    "    else:\n",
    "        answer = raw.strip()\n",
    "\n",
    "    # Remove duplicate citations but preserve order\n",
    "    unique_citations = list(dict.fromkeys(citations))\n",
    "\n",
    "    return answer, unique_citations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 — Process Query (Top-level function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query: str, top_k: int = TOP_K):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # Detect intent\n",
    "    intent = detect_intent_simple(query)\n",
    "    print(f\"Detected intent: {intent}\\n\")\n",
    "\n",
    "    # Retrieve top-k chunks\n",
    "    retrieved = vector_store.similarity_search(query, k=top_k)\n",
    "    print(f\"Retrieved {len(retrieved)} chunks.\\n\")\n",
    "\n",
    "    # Get synthesized answer\n",
    "    answer, citations = synthesize_topic_wise(llm, tokenizer, retrieved, query, intent)\n",
    "\n",
    "    # Print answer\n",
    "    print(\"--- ANSWER ---\\n\")\n",
    "    print(answer)\n",
    "\n",
    "    # Print citations\n",
    "    print(\"\\n--- CITATIONS ---\")\n",
    "    for c in citations:\n",
    "        print(c)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 — Next steps & utilities\n",
    "- If you'd like, I can:\n",
    "  - Add a small example `knowledge_base.json` with 6–8 representative entries so you can run the notebook immediately.\n",
    "  - Add a Dockerfile and resource guidance for running Llama 3.2 3B locally.\n",
    "  - Convert this notebook into a single `.py` module with CLI and unit tests.\n",
    "\n",
    "Tell me which of those you'd like next; I'll add it directly into the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Query: What interventions can I use for speeding in a school zone?\n",
      "Detected intent: find_intervention\n",
      "\n",
      "Retrieved 5 chunks.\n",
      "\n",
      "--- ANSWER ---\n",
      "\n",
      "- If the answer is not a simple \"yes\" or \"no\", provide a structured response.\n",
      "\n",
      "### 1) Problem Interpretation\n",
      "\n",
      "Speeding in school zones is a significant concern, as it poses a risk to the safety of children and pedestrians. The problem can be attributed to various factors, including inadequate infrastructure, lack of enforcement, and insufficient speed limit enforcement.\n",
      "\n",
      "### 2) Applicable IRC Clauses / Sources\n",
      "\n",
      "According to IRC:99-2018 - Clause 3.1.1.1, speed humps are a recommended traffic calming measure for reducing vehicle speeds in school zones.\n",
      "\n",
      "### 3) Topic-wise Recommended Interventions\n",
      "\n",
      "Based on the provided context, the following interventions can be used to address speeding in school zones:\n",
      "\n",
      "*   Speed Humps in School Zones (IRC:99-2018 - Clause 3.1.1.1)\n",
      "*   School Zone Enforcement Cameras (MoRTH Inspired, int-school_20)\n",
      "*   Safe School Zone Design (MoRTH Inspired, int-ped_16)\n",
      "*   Speed Limit 20/30 kmph Zones (WHO Inspired, int-school_02)\n",
      "\n",
      "### 4) Why This Works\n",
      "\n",
      "These interventions work by:\n",
      "\n",
      "*   Reducing vehicle speeds to safe walking speeds (Speed Humps)\n",
      "*   Ensuring compliance with speed and parking rules (School Zone Enforcement Cameras)\n",
      "*   Reducing vehicle speeds and protecting children (Safe School Zone Design)\n",
      "*   Reducing crash severity for children (Speed Limit 20/30 kmph Zones)\n",
      "\n",
      "### 5) Step-by-Step Fix Guide\n",
      "\n",
      "To implement these interventions, follow these steps:\n",
      "\n",
      "1.  Conduct a thorough risk assessment to identify areas with high speeding incidents.\n",
      "2.  Install speed humps or speed limit 20/30 kmph zones in identified areas.\n",
      "3.  Install school zone enforcement cameras in accident-prone or high-volume school corridors.\n",
      "4.  Design and implement safe school zone designs, including traffic calming measures, signage, fencing, and crossings.\n",
      "5.  Ensure community involvement and enforcement to maintain compliance with speed and parking rules.\n",
      "\n",
      "### 6) Estimated Cost (label as ESTIMATE if not in context)\n",
      "\n",
      "The estimated cost of implementing these interventions varies depending on the specific measures and location. However, a rough estimate can be provided as follows:\n",
      "\n",
      "*   Speed Humps: ₹500,000 - ₹1,000,000 per km\n",
      "*   School Zone Enforcement Cameras: ₹200,000 - ₹500,000 per camera\n",
      "*   Safe School Zone Design: ₹1,000,000 - ₹2,000,000 per km\n",
      "*   Speed Limit 20/30 kmph Zones: ₹100,000 - ₹200,000 per km\n",
      "\n",
      "Total estimated cost: ₹2,800,000 - ₹5,200,000\n",
      "\n",
      "### 7) Compliance Check\n",
      "\n",
      "To ensure compliance with these interventions, regular monitoring and enforcement are necessary. This can be achieved through:\n",
      "\n",
      "*   Regular inspections of speed humps and speed limit zones\n",
      "*   Monitoring of school zone enforcement cameras\n",
      "*   Community engagement and education on speed limits and parking rules\n",
      "*   Regular patrols by law enforcement agencies\n",
      "\n",
      "### 8) Final Summary\n",
      "\n",
      "Speeding in school zones is a significant concern that can be addressed through a combination of interventions, including speed humps, school zone enforcement cameras, safe school zone design, and speed limit 20/30 kmph zones. These interventions work by reducing vehicle speeds, ensuring compliance with speed and parking rules, and protecting children. By implementing these interventions and ensuring compliance, we can improve road safety in school zones.\n",
      "\n",
      "--- CITATIONS ---\n",
      "[WHO Inspired, int-school_08]\n",
      "[MoRTH Inspired, int-school_20]\n",
      "[MoRTH Inspired, int-ped_16]\n",
      "[WHO Inspired, int-school_02]\n",
      "[IRC:99-2018 - Clause 3.1.1.1, std-45]\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_query(\"What interventions can I use for speeding in a school zone?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Query: My road markings are faded and not retro-reflective. What's the rule for that?\n",
      "Detected intent: find_standard\n",
      "\n",
      "Retrieved 5 chunks.\n",
      "\n",
      "--- ANSWER ---\n",
      "\n",
      "- If the answer is not clear, say: \"Cannot answer from provided knowledge base.\"\n",
      "\n",
      "### 1) Problem Interpretation\n",
      "\n",
      "The problem at hand is faded and non-retro-reflective road markings, which can lead to reduced visibility and increased risk of accidents, especially at night or in low-light conditions.\n",
      "\n",
      "### 2) Applicable IRC Clauses / Sources\n",
      "\n",
      "The problem is related to the standards for road markings, specifically Clause 4.2 of IRC:35-2015, which addresses the maintenance and replacement of traffic lane line markings, and Clause 2.7 of IRC:35-2015, which outlines the requirements for retro-reflective sheeting on road signs.\n",
      "\n",
      "### 3) Topic-wise Recommended Interventions\n",
      "\n",
      "Based on the provided context, the recommended interventions for faded and non-retro-reflective road markings are:\n",
      "\n",
      "* Lane Marking Refurbishment (as per PWD Inspired, int-int_16): Repainting faded lane markings for clarity.\n",
      "* Replacement of retro-reflective sheeting on road signs (as per IRC:67-2022 - Clause 14.6.22, std-7): Ensuring the retro-reflective sheeting is weather-resistant, colorfast, and free from defects.\n",
      "\n",
      "### 4) Why This Works\n",
      "\n",
      "These interventions work because they address the root causes of the problem: reduced visibility and lack of retro-reflection. By repainting faded lane markings and ensuring the retro-reflective sheeting on road signs is of high quality, we can improve the visibility of road markings and reduce the risk of accidents.\n",
      "\n",
      "### 5) Step-by-Step Fix Guide\n",
      "\n",
      "1. Identify the faded and non-retro-reflective road markings and assess their condition.\n",
      "2. Repaint the faded lane markings using a high-quality paint and ensure proper application.\n",
      "3. Inspect the retro-reflective sheeting on road signs and replace it if necessary, ensuring it meets the standards outlined in IRC:67-2022 - Clause 14.6.22, std-7.\n",
      "4. Verify that the retro-reflective sheeting is weather-resistant, colorfast, and free from defects.\n",
      "\n",
      "### 6) Estimated Cost (label as ESTIMATE if not in context)\n",
      "\n",
      "Cannot answer from provided knowledge base.\n",
      "\n",
      "### 7) Compliance Check\n",
      "\n",
      "To ensure compliance with the relevant standards, it is essential to:\n",
      "\n",
      "* Verify that the retro-reflective sheeting meets the standards outlined in IRC:67-2022 - Clause 14.6.22, std-7.\n",
      "* Ensure that the lane markings are repainted using a high-quality paint and meet the standards outlined in IRC:35-2015 - Clause 4.2, std-28.\n",
      "\n",
      "### 8) Final Summary\n",
      "\n",
      "Faded and non-retro-reflective road markings can lead to reduced visibility and increased risk of accidents. To address this issue, it is recommended to repaint faded lane markings and ensure the retro-reflective sheeting on road signs is of high quality. This can be achieved through the implementation of the Lane Marking Refurbishment and Replacement of retro-reflective sheeting interventions.\n",
      "\n",
      "--- CITATIONS ---\n",
      "[IRC:35-2015 - Clause 2.7, std-21]\n",
      "[IRC:35-2015 - Clause 2.7, std-25]\n",
      "[PWD Inspired, int-int_16]\n",
      "[IRC:67-2022 - Clause 14.6.22, std-7]\n",
      "[IRC:35-2015 - Clause 4.2, std-28]\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_query(\"My road markings are faded and not retro-reflective. What's the rule for that?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
