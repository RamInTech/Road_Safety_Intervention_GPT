{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merged RAG system — Road Safety Intervention GPT\n",
    "\n",
    "Options selected by user:\n",
    "- Citation style: Strict Metadata Citation (Option A)\n",
    "- Output format: Topic-wise + Structured (Option 1)\n",
    "\n",
    "This is a single-file, production-ready Python module that:\n",
    "- Loads a knowledge_base.json (expects each item to have `full_text` and `metadata`)\n",
    "- Splits long texts into chunks and preserves metadata for each chunk\n",
    "- Builds a FAISS vectorstore with SentenceTransformer embeddings\n",
    "- Loads a local Llama-style model via Hugging Face (token placeholder)\n",
    "- Performs intent detection\n",
    "- Retrieves top-k chunks\n",
    "- Synthesizes a topic-wise + structured answer (Problem, IRC Clauses, Interventions grouped by topic, Step-by-step fix, Cost estimate, Compliance check)\n",
    "- Emits strict metadata citations in the form: [IRC:67-2022, Clause 12.3] or using metadata fields `source_reference` / `id` when available\n",
    "\n",
    "USAGE:\n",
    "- Put this file next to your `knowledge_base.json`.\n",
    "- Set HF_TOKEN and, if needed, adjust device settings.\n",
    "- Run `python Merged_RAG_Road_Safety_GPT.py` or import the functions in a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ram/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# --- third-party libs ---\n",
    "try:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "except Exception as e:\n",
    "    raise ImportError(\"Please install dependencies: langchain, langchain-community. Error: {}\".format(e))\n",
    "\n",
    "try:\n",
    "    from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "    import torch\n",
    "except Exception:\n",
    "    raise ImportError(\"Please install transformers and torch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Config\n",
    "# ---------------------------\n",
    "JSON_FILE_PATH = \"knowledge_base.json\"  # expected: list of {full_text: str, metadata: dict}\n",
    "EMBEDDING_MODEL = \"BAAI/bge-large-en-v1.5\"\n",
    "HF_MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"  # change if you use another LLM\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", None) # set your HF token here or via env variable\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else (\"mps\" if getattr(torch.backends, 'mps', False) and torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Retrieval settings\n",
    "CHUNK_SIZE = 600\n",
    "CHUNK_OVERLAP = 120\n",
    "TOP_K = 5\n",
    "\n",
    "# Cost estimation defaults (simple heuristic if cost metadata absent)\n",
    "COST_RATE_PER_MAN_DAY = 1500  # ₹ per man-day (example)\n",
    "WORK_DAYS_FOR_MINOR_FIX = 1\n",
    "WORK_DAYS_FOR_MEDIUM_FIX = 3\n",
    "WORK_DAYS_FOR_MAJOR_FIX = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Helper: load and chunk docs (preserve metadata)\n",
    "# ---------------------------\n",
    "\n",
    "def load_and_chunk_documents(file_path: str) -> Tuple[List[str], List[Dict[str, Any]]]:\n",
    "    \"\"\"Load JSON and split `full_text` into chunks while copying metadata for each chunk.\n",
    "\n",
    "    Returns: (chunks, chunk_metadatas)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"{file_path} not found. Place your knowledge base JSON in the same directory.\")\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    chunks = []\n",
    "    metadatas = []\n",
    "\n",
    "    for item in data:\n",
    "        full_text = item.get('full_text') or item.get('text') or \"\"\n",
    "        metadata = item.get('metadata', {})\n",
    "\n",
    "        if not full_text:\n",
    "            # skip empty\n",
    "            continue\n",
    "\n",
    "        splits = text_splitter.split_text(full_text)\n",
    "        for s in splits:\n",
    "            chunks.append(s)\n",
    "            # deep copy of metadata to avoid accidental mutation\n",
    "            m = dict(metadata)\n",
    "            # Ensure required citation fields exist\n",
    "            m.setdefault('id', item.get('id', m.get('id', 'N/A')))\n",
    "            m.setdefault('intervention_name', item.get('intervention_name', m.get('intervention_name', 'N/A')))\n",
    "            m.setdefault('source_reference', m.get('source_reference', m.get('source_reference', 'N/A')))\n",
    "            # Optional: a lightweight 'topic' field to help grouping\n",
    "            m.setdefault('topic', m.get('topic', infer_topic_from_text(s)))\n",
    "            metadatas.append(m)\n",
    "\n",
    "    return chunks, metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Simple topic inference fallback\n",
    "# ---------------------------\n",
    "\n",
    "def infer_topic_from_text(text: str) -> str:\n",
    "    \"\"\"Handy fallback: infer simple topic labels from the chunk text.\n",
    "    This is lightweight and heuristic — you can replace with a classifier later.\n",
    "    \"\"\"\n",
    "    t = text.lower()\n",
    "    if any(k in t for k in ['speed hump', 'speedbreaker', 'speed hump', 'rumble', 'hump', 'hump/']):\n",
    "        return 'Traffic Calming'\n",
    "    if any(k in t for k in ['sign', 'signage', 'stop sign', 'warning sign', 'retro-reflect']):\n",
    "        return 'Signing & Marking'\n",
    "    if any(k in t for k in ['pedestrian', 'zebra', 'crosswalk', 'footpath']):\n",
    "        return 'Pedestrian Facilities'\n",
    "    if any(k in t for k in ['lighting', 'street light', 'illumination']):\n",
    "        return 'Lighting'\n",
    "    if any(k in t for k in ['barrier', 'guardrail', 'crash barrier']):\n",
    "        return 'Road Restraint Systems'\n",
    "    if any(k in t for k in ['speed', 'enforcement', 'camera', 'radar']):\n",
    "        return 'Enforcement & Monitoring'\n",
    "    return 'General'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Build vectorstore\n",
    "# ---------------------------\n",
    "\n",
    "def build_vector_store(chunks: List[str], metadatas: List[Dict[str, Any]]):\n",
    "    print(\"[+] Building embedding model and vector store...\")\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "    store = FAISS.from_texts(chunks, embeddings, metadatas=metadatas)\n",
    "    print(f\"[+] Vector store created with {len(chunks)} chunks.\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Load LLM\n",
    "# ---------------------------\n",
    "\n",
    "def load_llm(model_name: str = HF_MODEL_NAME, token: str = HF_TOKEN):\n",
    "    if token == \"YOUR_HF_TOKEN_HERE\" or not token:\n",
    "        print(\"WARNING: HF_TOKEN not set. Set HF_TOKEN env var or edit the script to provide one.\")\n",
    "\n",
    "    print(f\"[+] Loading model: {model_name} on device {DEVICE} (this may take a while)...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\" if DEVICE == 'cuda' else None,\n",
    "        torch_dtype=torch.bfloat16 if DEVICE == 'cuda' else None,\n",
    "        trust_remote_code=True,\n",
    "        use_auth_token=token if token else None\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    llm = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=900,\n",
    "        do_sample=False\n",
    "    )\n",
    "    print(\"[+] LLM pipeline ready.\")\n",
    "    return llm, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Intent detection (simple rule-based). Expandable.\n",
    "# ---------------------------\n",
    "\n",
    "def detect_intent(query: str) -> str:\n",
    "    q = query.lower()\n",
    "    if any(x in q for x in ['cost', 'estimate', 'price', 'how much']):\n",
    "        return 'cost_estimate'\n",
    "    if any(x in q for x in ['fix', 'intervention', 'solution', 'what should i do', 'how to fix']):\n",
    "        return 'find_intervention'\n",
    "    if any(x in q for x in ['standard', 'specification', 'clause', 'irc', 'rule', 'compliance']):\n",
    "        return 'find_standard'\n",
    "    if any(x in q for x in ['compare', 'difference between', 'vs ', 'v/s']):\n",
    "        return 'compare_interventions'\n",
    "    if any(x in q for x in ['quiz', 'test me', 'questions']):\n",
    "        return 'request_quiz'\n",
    "    return 'ask_question'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Utility: Format strict metadata citation\n",
    "# ---------------------------\n",
    "\n",
    "def format_citation(metadata: Dict[str, Any]) -> str:\n",
    "    \"\"\"Return a strict metadata citation string.\n",
    "    Preferred fields (in order): 'irc_clause' (string), 'source_reference', 'id', 'intervention_name'.\n",
    "\n",
    "    Examples:\n",
    "      [IRC:67-2022, Clause 12.3]\n",
    "      [Source: Stop_Sign_Std_01]\n",
    "    \"\"\"\n",
    "    if not metadata:\n",
    "        return '[Source: N/A]'\n",
    "\n",
    "    if 'irc_clause' in metadata and metadata['irc_clause']:\n",
    "        return f\"[{metadata['irc_clause']}]\"\n",
    "\n",
    "    parts = []\n",
    "    if metadata.get('source_reference') and metadata['source_reference'] != 'N/A':\n",
    "        parts.append(str(metadata['source_reference']))\n",
    "    if metadata.get('id') and metadata['id'] != 'N/A':\n",
    "        parts.append(str(metadata['id']))\n",
    "    if metadata.get('intervention_name') and metadata['intervention_name'] != 'N/A':\n",
    "        parts.append(str(metadata['intervention_name']))\n",
    "\n",
    "    if parts:\n",
    "        return '[' + ', '.join(parts) + ']'\n",
    "\n",
    "    return '[Source: N/A]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Synthesize topic-wise + structured answer\n",
    "# ---------------------------\n",
    "\n",
    "def synthesize_answer(llm, tokenizer, retrieved_docs: List[Any], query: str, intent: str) -> Tuple[str, List[str]]:\n",
    "    \"\"\"Create a single prompt that instructs the LLM to produce topic-wise grouped output, with strict metadata citations.\n",
    "\n",
    "    Returns: (answer_text, list_of_used_citations)\n",
    "    \"\"\"\n",
    "    # Prepare context: for each retrieved doc we provide:\n",
    "    # - metadata fields (id, source_reference, irc_clause if present, topic)\n",
    "    # - text chunk\n",
    "\n",
    "    context_blocks = []\n",
    "    citations_used = []\n",
    "\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        meta = doc.metadata if hasattr(doc, 'metadata') else doc.get('metadata', {})\n",
    "        text = doc.page_content if hasattr(doc, 'page_content') else doc.get('text', '')\n",
    "        citation = format_citation(meta)\n",
    "        citations_used.append(citation)\n",
    "        block = f\"---\\nCitation: {citation}\\nTopic: {meta.get('topic','N/A')}\\nMetadata: {json.dumps(meta)}\\nContent: {text}\\n\"\n",
    "        context_blocks.append(block)\n",
    "\n",
    "    context_text = \"\\n\".join(context_blocks)\n",
    "\n",
    "    # Structured template with topic-wise grouping instruction\n",
    "    system_prompt = (\n",
    "        \"You are an Expert Road Safety Analyst. Use ONLY the provided context blocks \"\n",
    "        \"to answer. Do NOT hallucinate other standards. \"\n",
    "        \"Produce a TOPIC-WISE and STRUCTURED response with the sections below. \"\n",
    "        \"Every factual claim must include an inline citation using the citation labels provided like [IRC:67-2022, Clause 12.3] or [Source_ID]. \"\n",
    "    )\n",
    "\n",
    "    # The structure we enforce (Option 1)\n",
    "    user_instructions = f\"\"\"\n",
    "Context Blocks:\n",
    "{context_text}\n",
    "\n",
    "User Query:\n",
    "{query}\n",
    "\n",
    "Task: Produce an answer with the following exact structure. Use the same section headings and keep the order.\n",
    "\n",
    "### 1) Problem Interpretation\n",
    "- Short paraphrase of what the problem/query means (1-3 lines).\n",
    "\n",
    "### 2) Applicable IRC Clauses / Sources\n",
    "- List the exact clauses or sources found in the context that apply. Use the provided citation labels exactly.\n",
    "\n",
    "### 3) Topic-wise Recommended Interventions\n",
    "- Group interventions into topics (e.g., 'Engineering Measures', 'Enforcement Measures', 'Education & Awareness').\n",
    "- For each topic include:\n",
    "  - Short description of the intervention\n",
    "  - Exact actionable parameters (dimensions, placement, materials) if present in the context\n",
    "  - Inline citation(s) for each claim\n",
    "\n",
    "### 4) Why This Works (engineering justification)\n",
    "- For each topic, provide 1-2 lines justification referencing context.\n",
    "\n",
    "### 5) Step-by-Step Fix Guide\n",
    "- Provide a numbered 3-7 step procedure to implement the top recommended intervention(s). Cite sources where steps derive from standards.\n",
    "\n",
    "### 6) Estimated Cost (if applicable)\n",
    "- Provide a simple cost band: Low / Medium / High with numeric range in ₹ and state assumptions. If precise cost info is not in the context, estimate using internal heuristics and label them as ESTIMATE.\n",
    "\n",
    "### 7) Compliance Check (if user asked about compliance or if standards are in context)\n",
    "- State 'Compliant' or 'Not Compliant' with short reasoning and cite clause(s).\n",
    "\n",
    "### 8) Final Summary\n",
    "- 3–5 line concise summary and the top 1 recommended intervention.\n",
    "\n",
    "Notes:\n",
    "- If the context does NOT contain necessary information, explicitly state: \"Cannot answer from provided knowledge base.\" and cite the retrieved sources used to determine absence.\n",
    "- Avoid adding any new standards beyond what appears in the context blocks.\n",
    "\"\"\"\n",
    "\n",
    "    full_prompt = system_prompt + '\\n\\n' + user_instructions\n",
    "\n",
    "    # Generate\n",
    "    try:\n",
    "        output = llm(full_prompt, num_return_sequences=1)\n",
    "        raw = output[0]['generated_text'] if isinstance(output, list) else str(output)\n",
    "\n",
    "        # If the pipeline returns prompt + output, attempt to chop — models vary in behavior.\n",
    "        if raw.startswith(full_prompt):\n",
    "            answer = raw[len(full_prompt):].strip()\n",
    "        else:\n",
    "            answer = raw.strip()\n",
    "\n",
    "        # Deduplicate citations for return\n",
    "        unique_citations = sorted(set(citations_used), key=lambda x: citations_used.index(x))\n",
    "        return answer, unique_citations\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: LLM generation failed: {e}\", []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Query processing top-level\n",
    "# ---------------------------\n",
    "\n",
    "def process_query(vector_store, llm, tokenizer, query: str, top_k: int = TOP_K) -> None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    intent = detect_intent(query)\n",
    "    print(f\"Detected intent: {intent}\\n\")\n",
    "\n",
    "    # Retrieve\n",
    "    retrieved = vector_store.similarity_search(query, k=top_k)\n",
    "    print(f\"Retrieved {len(retrieved)} chunks (top {top_k}).\\n\")\n",
    "\n",
    "    if not retrieved:\n",
    "        print(\"No relevant documents found in vector store. Make sure knowledge_base.json has content and was indexed.\")\n",
    "        return\n",
    "\n",
    "    answer, citations = synthesize_answer(llm, tokenizer, retrieved, query, intent)\n",
    "\n",
    "    print(\"--- Generated Answer ---\\n\")\n",
    "    print(answer)\n",
    "    print(\"\\n--- Citations used ---\")\n",
    "    for c in citations:\n",
    "        print(c)\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged RAG — Road Safety Intervention GPT (Topic-wise + Structured, strict metadata citations)\n",
      "[+] Building embedding model and vector store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7q/cwlrl67n1p3fsjh24d6dk5r00000gn/T/ipykernel_23100/2373843269.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Vector store created with 298 chunks.\n",
      "[+] Loading model: meta-llama/Llama-3.2-3B-Instruct on device mps (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ram/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.15s/it]\n",
      "Device set to use mps:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] LLM pipeline ready.\n",
      "\n",
      "Ready. Type your question (type \"exit\" to quit).\n",
      "\n",
      "============================================================\n",
      "Query: My road markings are faded and not retro-reflective. What's the rule for that?\n",
      "\n",
      "Detected intent: find_standard\n",
      "\n",
      "Retrieved 5 chunks (top 5).\n",
      "\n",
      "--- Generated Answer ---\n",
      "\n",
      "- Use the exact wording of the context blocks for citations and intervention names.\n",
      "\n",
      "---\n",
      "\n",
      "### 1) Problem Interpretation\n",
      "The user's road markings are faded and lack retro-reflectivity, which poses a visibility issue, especially at night or in low-light conditions.\n",
      "\n",
      "### 2) Applicable IRC Clauses / Sources\n",
      "- [IRC:35-2015 - Clause 2.7, std-21, Word Message TRAM & BUS ONLY Marking]\n",
      "- [IRC:35-2015 - Clause 2.7, std-25, Direction Information NO ENTRY Marking]\n",
      "- [IRC:67-2022 - Clause 14.6.22, std-7, U-Turn Prohibited Sign]\n",
      "- [IRC:35-2015 - Clause 2.2, std-24, Straight Arrow Marking]\n",
      "\n",
      "### 3) Topic-wise Recommended Interventions\n",
      "#### Signing & Marking\n",
      "- **Lane Marking Refurbishment**: Improves lane discipline and reduces side-swipe crashes by repainting faded lane markings for clarity. Requires frequent maintenance. [PWD Inspired, int-int_16]\n",
      "  - Actionable parameters: Any intersection with poor visibility.\n",
      "  - Inline citation: [PWD Inspired, int-int_16]\n",
      "\n",
      "#### Signing & Marking\n",
      "- **U-Turn Prohibited Sign**: Ensures retro-reflective sheeting is weather-resistant, colorfast, and free from defects. [IRC:67-2022 - Clause 14.6.22, std-7]\n",
      "  - Actionable parameters: Minimum coefficient of retro-reflection in accordance with ASTM D 4956 standards.\n",
      "  - Inline citation: [IRC:67-2022 - Clause 14.6.22, std-7]\n",
      "\n",
      "#### Signing & Marking\n",
      "- **Straight Arrow Marking**: Provides clear direction for drivers to take a mandatory turn. [IRC:35-2015 - Clause 2.2, std-24]\n",
      "  - Actionable parameters: 3.5 m length (AM01) for speeds below 50 km/h, 5 m length (AM08) for speeds between 51-100 km/h, and 9 m length (AM08) for speeds above 100 km/h.\n",
      "  - Inline citation: [IRC:35-2015 - Clause 2.2, std-24]\n",
      "\n",
      "### 4) Why This Works (engineering justification)\n",
      "- Lane Marking Refurbishment improves visibility and reduces crashes by ensuring clear lane markings.\n",
      "- U-Turn Prohibited Sign ensures retro-reflective sheeting is durable and effective.\n",
      "\n",
      "### 5) Step-by-Step Fix Guide\n",
      "1. Inspect the intersection for faded lane markings and poor visibility.\n",
      "2. Identify areas requiring refurbishment.\n",
      "3. Repaint faded lane markings with a highly durable thermoplastic pavement material.\n",
      "4. Ensure the new markings meet the specified dimensions and standards.\n",
      "5. Replace the U-Turn Prohibited Sign if its retro-reflective sheeting is worn or damaged.\n",
      "6. Replace the Straight Arrow Marking if it is faded or damaged.\n",
      "\n",
      "### 6) Estimated Cost (if applicable)\n",
      "- ESTIMATE: ₹50,000 - ₹200,000 (depending on the extent of refurbishment and replacement of signs).\n",
      "\n",
      "### 7) Compliance Check\n",
      "- Compliant with IRC standards for road markings and signs.\n",
      "\n",
      "### 8) Final Summary\n",
      "The top recommended intervention is Lane Marking Refurbishment, which improves visibility and reduces crashes. Estimated cost: ₹50,000 - ₹200,000.\n",
      "\n",
      "--- Citations used ---\n",
      "[IRC:35-2015 - Clause 2.7, std-21, Word Message TRAM & BUS ONLY Marking]\n",
      "[IRC:35-2015 - Clause 2.7, std-25, Direction Information NO ENTRY Marking]\n",
      "[PWD Inspired, int-int_16, Lane Marking Refurbishment]\n",
      "[IRC:67-2022 - Clause 14.6.22, std-7, U-Turn Prohibited Sign]\n",
      "[IRC:35-2015 - Clause 2.2, std-24, Straight Arrow Marking]\n",
      "\n",
      "============================================================\n",
      "\n",
      "Exiting.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# CLI / Example usage\n",
    "# ---------------------------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Merged RAG — Road Safety Intervention GPT (Topic-wise + Structured, strict metadata citations)\")\n",
    "\n",
    "    # 1. Load and chunk\n",
    "    chunks, metadatas = load_and_chunk_documents(JSON_FILE_PATH)\n",
    "\n",
    "    # 2. Build vector store\n",
    "    vector_store = build_vector_store(chunks, metadatas)\n",
    "\n",
    "    # 3. Load LLM\n",
    "    llm, tokenizer = load_llm()\n",
    "\n",
    "    # 4. Interactive loop\n",
    "    print('\\nReady. Type your question (type \"exit\" to quit).')\n",
    "    while True:\n",
    "        q = input('\\nYour query > ').strip()\n",
    "        if q.lower() in ('exit', 'quit'):\n",
    "            print('Exiting.')\n",
    "            break\n",
    "        if not q:\n",
    "            continue\n",
    "        process_query(vector_store, llm, tokenizer, q, top_k=TOP_K)\n",
    "\n",
    "# End of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
